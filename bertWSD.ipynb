{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertWSD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiiNk0hen963",
        "outputId": "19ea8493-ed42-4706-ffe1-05eca4646781"
      },
      "source": [
        "!pip install transformers\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_VPVknjcGuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4b2c60-4cff-4c8c-c75f-0bb451406018"
      },
      "source": [
        "pip install -q tf-models-official==2.3.0\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 840 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 58.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 49 kB/s \n",
            "\u001b[K     |████████████████████████████████| 679 kB 17.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 59.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 11.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.2 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izTEnodGjpiH"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks\n",
        "import pandas as pd\n",
        "import transformers as ppb\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8XkdwrjtXmx"
      },
      "source": [
        "with open(\"training.en-en.data\", \"r\",encoding='UTF-8') as read_file:\n",
        "    data = json.load(read_file)\n",
        "with open(\"training.en-en.gold\", \"r\",encoding='UTF-8') as read_file:\n",
        "    gold = json.load(read_file)\n",
        "with open(\"dev.en-en.data\", \"r\",encoding='UTF-8') as read_file:\n",
        "    dev_data = json.load(read_file)\n",
        "with open(\"dev.en-en.gold\", \"r\",encoding='UTF-8') as read_file:\n",
        "    dev_gold = json.load(read_file)\n",
        "with open(\"test.en-en.data\", \"r\",encoding='UTF-8') as read_file:\n",
        "    test_data = json.load(read_file)\n",
        "with open(\"test.en-en.gold\", \"r\",encoding='UTF-8') as read_file:\n",
        "    test_gold = json.load(read_file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXKNGfitJ458",
        "outputId": "ce35e17b-fbf6-41dd-d2c5-099ac163975e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1kBkulg_y8Q"
      },
      "source": [
        "labels_tmp= np.array([dat['tag'] for dat in gold])\n",
        "labels= [0 if x=='F' else 1 for x in labels_tmp]\n",
        "labels=np.asarray(labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r_UKS2MEYJr"
      },
      "source": [
        "labels_tmp_val= np.array([dat['tag'] for dat in dev_gold])\n",
        "labels_val= [0 if x=='F' else 1 for x in labels_tmp_val]\n",
        "labels_val=np.asarray(labels_val)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvYdcy1aEiUq"
      },
      "source": [
        "labels_tmp_test= np.array([dat['tag'] for dat in test_gold])\n",
        "labels_test= [0 if x=='F' else 1 for x in labels_tmp_test]\n",
        "labels_test=np.asarray(labels_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf31EIe8I5ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46db9c82-b1e1-42b1-f6bc-a64bb9a54acc"
      },
      "source": [
        "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
        "tf.io.gfile.listdir(gs_folder_bert)\n",
        "\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "# tokenizer =tokenizer_class.from_pretrained(pretrained_weights)\n",
        "tokenizer = bert.tokenization.FullTokenizer(\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
        "     do_lower_case=True)\n",
        "\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))\n",
        "def merge(lst1, lst2):\n",
        "    return [a + b[1:] for (a, b) in zip(lst1, lst2)]\n",
        "\n",
        "def encode_sentence(s):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "# df = pd.DataFrame(columns = 'label',data=[dat['tag'] for dat in gold])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9mdsZIw25pA"
      },
      "source": [
        "def prepareData(data):\n",
        "    sentence1=[]\n",
        "    sentence2=[]\n",
        "    paraphrased1_encode=[]\n",
        "    paraphrased2_encode=[]\n",
        "    for i in range(len(data)):\n",
        "        dat=data[i]\n",
        "        sent1=dat['sentence1']\n",
        "        sent2=dat['sentence2']\n",
        "        \n",
        "        wordInFirst=sent1[int(dat['start1']):int(dat['end1'])]\n",
        "        wordInSecond=sent2[int(dat['start2']):int(dat['end2'])]\n",
        "        newSent1=sent1[0:int(dat['start1'])]+'[TGT] '+wordInFirst+' [TGT]'+sent1[int(dat['end1']):len(sent1)]\n",
        "        newSent2=sent2[0:int(dat['start2'])]+'[TGT] '+ wordInSecond+' [TGT]'+sent2[int(dat['end2']):len(sent2)]\n",
        "        sentence1.append(encode_sentence(newSent1))\n",
        "        sentence2.append(encode_sentence(newSent2))\n",
        "\n",
        "    return sentence1,sentence2\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hRJyQHEoQ7f"
      },
      "source": [
        "sentence1_train,sentence2_train=prepareData(data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj_7kQTtF5HK"
      },
      "source": [
        "sentence1_val,sentence2_val=prepareData(dev_data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mWql3rGZk6"
      },
      "source": [
        "sentence1_test,sentence2_test=prepareData(test_data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akX0uJwXJEoS"
      },
      "source": [
        "def prepare_input(sentence1,sentence2):\n",
        "    sentence1=tf.ragged.constant(sentence1)\n",
        "    sentence2=tf.ragged.constant(sentence2)\n",
        "\n",
        "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n",
        "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "\n",
        "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "    type_cls = tf.zeros_like(cls)\n",
        "    type_s1 = tf.ones_like(sentence1)\n",
        "    type_s2 = tf.ones_like(sentence2)\n",
        "    type_s2 = type_s2+1;\n",
        "    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "    print(type_s1.shape)\n",
        "    inputs = {\n",
        "        'input_word_ids': input_word_ids.to_tensor(),\n",
        "        'input_mask': input_mask,\n",
        "        'input_type_ids': input_type_ids}\n",
        "    return inputs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYuO4qfmizSz",
        "outputId": "ed71db5d-571d-4634-c7eb-ed7ad7ff3f2c"
      },
      "source": [
        "inputsTrain=prepare_input(sentence1_train,sentence2_train)\n",
        "inputsVal=prepare_input(sentence1_val,sentence2_val)\n",
        "inputsTest=prepare_input(sentence1_test,sentence2_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, None)\n",
            "(1000, None)\n",
            "(1000, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtWRW_jBfzgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13d59b7-960f-4556-ec26-535ba4e691fb"
      },
      "source": [
        "\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
        "\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "print(config_dict)\n",
        "# config_dict={'attention_probs_dropout_prob': 0.1,\n",
        "#  'hidden_act': 'relu',\n",
        "#  'hidden_dropout_prob': 0.1,\n",
        "#  'hidden_size': 768,\n",
        "#  'initializer_range': 0.02,\n",
        "#  'intermediate_size': 3072,\n",
        "#  'max_position_embeddings': 512,\n",
        "#  'num_attention_heads': 12,\n",
        "#  'num_hidden_layers': 12,\n",
        "#  'type_vocab_size': 2,\n",
        "#  'vocab_size': 30522}\n",
        "\n",
        "# bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "# print('test')\n",
        "# print(bert_config['initializer_range'])\n",
        "# print(bert.bert_models.get_transformer_encoder( bert_config ,sequence_length=100))\n",
        "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
        "    bert_config, num_labels=2)\n",
        "tf.keras.utils.plot_model(bert_classifier, show_shapes=True, dpi=48)\n",
        "checkpoint = tf.train.Checkpoint(model=bert_encoder)\n",
        "checkpoint.restore(\n",
        "    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()\n",
        "# print(bert_encoder.get_embedding_table())\n",
        "epochs = 6\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_data_size = len(labels)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "\n",
        "# creates an optimizer with learning rate schedule\n",
        "optimizer = nlp.optimization.create_optimizer(\n",
        "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "#                                                     patience=2,\n",
        "#                                                     mode='min')\n",
        "bert_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "bert_classifier.fit(\n",
        "      inputsTrain, labels,\n",
        "      validation_split=0.1,\n",
        "      shuffle=True,\n",
        "      batch_size=32,\n",
        "      # callbacks=[early_stopping],\n",
        "      epochs=epochs)\n",
        "export_dir='./saved_model'\n",
        "tf.saved_model.save(bert_classifier, export_dir=export_dir)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 30522}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "225/225 [==============================] - 240s 1s/step - loss: 0.6585 - accuracy: 0.5861 - val_loss: 0.5892 - val_accuracy: 0.7262\n",
            "Epoch 2/6\n",
            "225/225 [==============================] - 236s 1s/step - loss: 0.4761 - accuracy: 0.7726 - val_loss: 0.4255 - val_accuracy: 0.8200\n",
            "Epoch 3/6\n",
            "225/225 [==============================] - 237s 1s/step - loss: 0.3033 - accuracy: 0.8811 - val_loss: 0.4544 - val_accuracy: 0.8263\n",
            "Epoch 4/6\n",
            "225/225 [==============================] - 237s 1s/step - loss: 0.1757 - accuracy: 0.9425 - val_loss: 0.5467 - val_accuracy: 0.8213\n",
            "Epoch 5/6\n",
            "225/225 [==============================] - 238s 1s/step - loss: 0.1080 - accuracy: 0.9661 - val_loss: 0.6938 - val_accuracy: 0.8225\n",
            "Epoch 6/6\n",
            "225/225 [==============================] - 237s 1s/step - loss: 0.0812 - accuracy: 0.9761 - val_loss: 0.7555 - val_accuracy: 0.8150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as self_attention_layer_call_fn, self_attention_layer_call_and_return_conditional_losses, attention_output_layer_call_fn, attention_output_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn while saving (showing 5 of 840). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7sUoobqjj_0"
      },
      "source": [
        "bert_classifier_model = tf.keras.models.load_model('drive/MyDrive/saved_model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRrP-kal2f3k"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivkXKLVLqwBA"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i69X6_Spg6J",
        "outputId": "5a6f7c28-2d6a-4509-88c6-40ba7150e0c5"
      },
      "source": [
        "result= bert_classifier.predict(inputsTest)\n",
        "result = tf.argmax(result,axis=1).numpy()\n",
        "# print(test_labels.shape)\n",
        "print(accuracy_score(labels_test,result))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.855\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}