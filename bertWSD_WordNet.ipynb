{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertWSD_WordNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiiNk0hen963",
        "outputId": "228c196c-3f7a-4867-8dc4-61c0ad23e4cb"
      },
      "source": [
        "!pip install transformers\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_VPVknjcGuy"
      },
      "source": [
        "!pip install -q tf-models-official==2.3.0\n",
        "import json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8XkdwrjtXmx"
      },
      "source": [
        "with open(\"training.en-en.data\", \"r\",encoding='UTF-8') as read_file:\n",
        "    data = json.load(read_file)\n",
        "with open(\"training.en-en.gold\", \"r\",encoding='UTF-8') as read_file:\n",
        "    gold = json.load(read_file)\n",
        "with open(\"dev.en-en.data\", \"r\",encoding='UTF-8') as read_file:\n",
        "    dev_data = json.load(read_file)\n",
        "with open(\"dev.en-en.gold\", \"r\",encoding='UTF-8') as read_file:\n",
        "    dev_gold = json.load(read_file)\n",
        "with open(\"test.en-en.data\", \"r\",encoding='UTF-8') as read_file:\n",
        "    test_data = json.load(read_file)\n",
        "with open(\"test.en-en.gold\", \"r\",encoding='UTF-8') as read_file:\n",
        "    test_gold = json.load(read_file)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8NJmyAT_2TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ade6c09-1b19-45c8-b1c0-22ffbe19a969"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHdTk221YqGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3542569-80bf-4897-89df-c116c843137c"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize   \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))   "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekOMkZQCfQfm"
      },
      "source": [
        "def map_pos(wordnet,pos):\n",
        "  if pos=='NOUN':\n",
        "    return wordnet.NOUN\n",
        "  elif pos=='VERB':\n",
        "    return wordnet.VERB\n",
        "  elif pos=='ADJ':\n",
        "    return wordnet.ADJ\n",
        "  elif pos=='ADV':\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return pos"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lou7dfo0sT0m"
      },
      "source": [
        "def sort ( dict1):\n",
        "  sorted_dict = {}\n",
        "  sorted_values = sorted(dict1.values()) \n",
        "  for i in sorted_values:\n",
        "      for k in dict1.keys():\n",
        "          if dict1[k] == i:\n",
        "              sorted_dict[k] = dict1[k]\n",
        "              break\n",
        "  return sorted_dict"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j59KQMAIZOet"
      },
      "source": [
        "def get_wordnet_results(data):\n",
        "  sents1_path=[]\n",
        "  sents2_path=[]\n",
        "  punctuations = '''!()-—[]{};:'”\"\\,<>./?@#$%^&*_~'''\n",
        "  count=0\n",
        "  for dat in data:\n",
        "    sent1=dat['sentence1']\n",
        "    sent2=dat['sentence2']\n",
        "    wordInFirst=sent1[int(dat['start1']):int(dat['end1'])].lower()\n",
        "    wordInSecond=sent2[int(dat['start2']):int(dat['end2'])].lower()\n",
        "    pos=dat['pos']\n",
        "    lemma=dat['lemma']\n",
        "    for ch in punctuations:\n",
        "      sent1= sent1.lower().replace(ch,' ')\n",
        "    for ch in punctuations:\n",
        "      sent2= sent2.lower().replace(ch,' ')\n",
        "    # print(sent1)\n",
        "    word_tokens1 = word_tokenize(sent1.lower())  \n",
        "    word_tokens2 = word_tokenize(sent2.lower())  \n",
        "    sent1 = [w for w in word_tokens1 if w in wordInFirst or (not w in stop_words )] \n",
        "    sent2 = [w for w in word_tokens2 if w in wordInSecond or ( not w in stop_words)]\n",
        "    pos= pos.split(\"\\t\")[0]\n",
        "    # print(pos)\n",
        "    pos_wn= map_pos(wordnet,pos)\n",
        "    # print(sent1)\n",
        "    # print(sent2)\n",
        "    # print(wordInSecond)\n",
        "    if count%1000==0:\n",
        "      print(count)\n",
        "    count+=1\n",
        "    result1=find_paths(sent1,lemma,wordInFirst,pos_wn)\n",
        "    result2=find_paths(sent2,lemma,wordInSecond,pos_wn)\n",
        "    result1= sort(result1)\n",
        "    result2= sort(result2)\n",
        "    sents1_path.append(result1)\n",
        "    sents2_path.append(result2)\n",
        "  return sents1_path,sents2_path"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fPwHyT-Ao3K"
      },
      "source": [
        "import numpy as np\n",
        "def find_paths(words, lemma , w, pos):\n",
        "  ind =words.index(w)\n",
        "  syns = wordnet.synsets(lemma,pos)\n",
        "  result={}\n",
        "  for syn in syns:\n",
        "    # print(syn.lexname())\n",
        "    sum=0\n",
        "    num=0\n",
        "    for i in range(len(words)):\n",
        "      word=words[i]\n",
        "      if i!=ind:\n",
        "        syns2= wordnet.synsets(word,pos)\n",
        "        # print(syns2)\n",
        "        min=10000\n",
        "        for syn2 in syns2:\n",
        "          path= syn.shortest_path_distance(syn2)\n",
        "          if path!=None:\n",
        "            if path<min:\n",
        "              min=path\n",
        "        if min<10000:\n",
        "          num=(np.abs(i-ind))\n",
        "          sum+=min*(num)\n",
        "    if num!=0:\n",
        "      result[syn.lexname()]=sum\n",
        "  return result\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1kBkulg_y8Q"
      },
      "source": [
        "labels_tmp= np.array([dat['tag'] for dat in gold])\n",
        "labels= [0 if x=='F' else 1 for x in labels_tmp]\n",
        "labels=np.asarray(labels)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r_UKS2MEYJr"
      },
      "source": [
        "labels_tmp_val= np.array([dat['tag'] for dat in dev_gold])\n",
        "labels_val= [0 if x=='F' else 1 for x in labels_tmp_val]\n",
        "labels_val=np.asarray(labels_val)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvYdcy1aEiUq"
      },
      "source": [
        "labels_tmp_test= np.array([dat['tag'] for dat in test_gold])\n",
        "labels_test= [0 if x=='F' else 1 for x in labels_tmp_test]\n",
        "labels_test=np.asarray(labels_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9mdsZIw25pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a967b129-6a5f-4fcf-fc5a-a158267d351a"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks\n",
        "import pandas as pd\n",
        "import transformers as ppb\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
        "tf.io.gfile.listdir(gs_folder_bert)\n",
        "\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "# tokenizer =tokenizer_class.from_pretrained(pretrained_weights)\n",
        "tokenizer = bert.tokenization.FullTokenizer(\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
        "     do_lower_case=True)\n",
        "\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))\n",
        "def merge(lst1, lst2):\n",
        "    return [a + b[1:] for (a, b) in zip(lst1, lst2)]\n",
        "\n",
        "def encode_sentence(s):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "# df = pd.DataFrame(columns = 'label',data=[dat['tag'] for dat in gold])\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D97c3HalMZyR"
      },
      "source": [
        "def prepareData(data):\n",
        "    sentence1=[]\n",
        "    sentence2=[]\n",
        "    paraphrased1_encode=[]\n",
        "    paraphrased2_encode=[]\n",
        "    # lemmas1=[]\n",
        "    # lemmas2=[]\n",
        "    types_s1=[]\n",
        "    types_s2=[]\n",
        "    result_wordnet1,result_wordnet2=get_wordnet_results(data)\n",
        "    count=0\n",
        "    for dat in data:\n",
        "        sent1=dat['sentence1']\n",
        "        sent2=dat['sentence2']\n",
        "        lemma=dat['lemma']\n",
        "        wordInFirst=sent1[int(dat['start1']):int(dat['end1'])]\n",
        "        wordInSecond=sent2[int(dat['start2']):int(dat['end2'])]\n",
        "        newSent1=sent1[0:int(dat['start1'])]+' [TGT] '+wordInFirst+' [TGT] '+sent1[int(dat['end1']):len(sent1)]\n",
        "        newSent2=sent2[0:int(dat['start2'])]+' [TGT] '+ wordInSecond+' [TGT] '+sent2[int(dat['end2']):len(sent2)]\n",
        "        sent1_wordnet=''\n",
        "        sent2_wordnet=''\n",
        "        for lexname in result_wordnet1[count]:\n",
        "          sent1_wordnet+=' [LX] '+lexname.split('.')[1]\n",
        "        for lexname in result_wordnet2[count]:\n",
        "          sent2_wordnet+=' [LX] '+lexname.split('.')[1]\n",
        "        newSent1=newSent1+sent1_wordnet\n",
        "        newSent2=newSent2+sent2_wordnet\n",
        "\n",
        "        # print(newSent1)\n",
        "        # print(newSent2)\n",
        "\n",
        "        encoded_sent1=encode_sentence(newSent1)\n",
        "        encoded_sent2=encode_sentence(newSent2)\n",
        "\n",
        "        # print(encoded_sent1)\n",
        "        # print(encoded_sent2)\n",
        "        sentence1.append(encoded_sent1)\n",
        "        sentence2.append(encoded_sent2)\n",
        "        encode_w1= encode_sentence(wordInFirst)\n",
        "        encode_w2= encode_sentence(wordInSecond)\n",
        "        count+=1\n",
        "\n",
        "    sentence1=tf.ragged.constant(sentence1)\n",
        "    sentence2=tf.ragged.constant(sentence2)\n",
        "\n",
        "    # paraphrased1_encode=tf.ragged.constant(paraphrased1_encode)\n",
        "    # paraphrased2_encode=tf.ragged.constant(paraphrased2_encode)\n",
        "\n",
        "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n",
        "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "    # input_para_ids = tf.concat([cls, paraphrased1_encode, paraphrased2_encode], axis=-1)\n",
        "\n",
        "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "    type_cls = tf.zeros_like(cls)\n",
        "    types_s1 = tf.ones_like(sentence1)\n",
        "    types_s2 = tf.ones_like(sentence2)\n",
        "    types_s2 = types_s2+1;\n",
        "    input_type_ids = tf.concat([type_cls, types_s1, types_s2], axis=-1).to_tensor()\n",
        "    inputs = {\n",
        "        'input_word_ids': input_word_ids.to_tensor(),\n",
        "        # 'input_para_ids': input_para_ids.to_tensor(),\n",
        "        'input_mask': input_mask,\n",
        "        'input_type_ids': input_type_ids}\n",
        "\n",
        "    return inputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiagd2L5M5mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4137086-8e64-4b14-9333-8d12706545ce"
      },
      "source": [
        "inputsTrain=prepareData(data)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS-0ErQEM9sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a37bc49-9765-44ef-b28a-bec70221d640"
      },
      "source": [
        "inputsVal=prepareData(dev_data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFo5shTQM-Yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ced6e8-5747-4ebb-eb16-2e7f6b743e01"
      },
      "source": [
        "inputsTest=prepareData(test_data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6W-3fcRJsf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadf502f-f577-41a5-b4ac-6377672bd2ae"
      },
      "source": [
        "lemmas1=[]\n",
        "lemmas2=[]\n",
        "for dat in data:\n",
        "  sent1=dat['sentence1']\n",
        "  sent2=dat['sentence2']\n",
        "  encoded_sent1=encode_sentence(sent1)\n",
        "  encoded_sent2=encode_sentence(sent2)\n",
        "  print(sent1)\n",
        "  print(encoded_sent1)\n",
        "  print(sent2)\n",
        "  print(encoded_sent2)\n",
        "  encode_lemma= encode_sentence('play')\n",
        "  lemmas1.append(encoded_sent1.index(encode_lemma[0]))\n",
        "  lemmas2.append(encoded_sent2.index(encode_lemma[0]))\n",
        "  break\n",
        "print(lemmas1)\n",
        "print(lemmas2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In that context of coordination and integration, Bolivia holds a key play in any process of infrastructure development.\n",
            "[1999, 2008, 6123, 1997, 12016, 1998, 8346, 1010, 11645, 4324, 1037, 3145, 2377, 1999, 2151, 2832, 1997, 6502, 2458, 1012, 102]\n",
            "A musical play on the same subject was also staged in Kathmandu for three days.\n",
            "[1037, 3315, 2377, 2006, 1996, 2168, 3395, 2001, 2036, 9813, 1999, 28045, 2005, 2093, 2420, 1012, 102]\n",
            "[12]\n",
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtWRW_jBfzgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287987c5-127e-4e33-d3a3-040f338cc7d9"
      },
      "source": [
        "\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
        "\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "print(config_dict)\n",
        "# config_dict={'attention_probs_dropout_prob': 0.1,\n",
        "#  'hidden_act': 'relu',\n",
        "#  'hidden_dropout_prob': 0.1,\n",
        "#  'hidden_size': 768,\n",
        "#  'initializer_range': 0.02,\n",
        "#  'intermediate_size': 3072,\n",
        "#  'max_position_embeddings': 512,\n",
        "#  'num_attention_heads': 12,\n",
        "#  'num_hidden_layers': 12,\n",
        "#  'type_vocab_size': 2,\n",
        "#  'vocab_size': 30522}\n",
        "\n",
        "# bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "# print('test')\n",
        "# print(bert_config['initializer_range'])\n",
        "# print(bert.bert_models.get_transformer_encoder( bert_config ,sequence_length=100))\n",
        "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
        "    bert_config, num_labels=2)\n",
        "tf.keras.utils.plot_model(bert_classifier, show_shapes=True, dpi=48)\n",
        "checkpoint = tf.train.Checkpoint(model=bert_encoder)\n",
        "checkpoint.restore(\n",
        "    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()\n",
        "# print(bert_encoder.get_embedding_table())\n",
        "epochs = 10\n",
        "batch_size = 16\n",
        "eval_batch_size = 16\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=2,\n",
        "                                                    mode='min')\n",
        "train_data_size = len(labels)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "\n",
        "# creates an optimizer with learning rate schedule\n",
        "optimizer = nlp.optimization.create_optimizer(\n",
        "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "bert_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "bert_classifier.fit(\n",
        "      inputsTrain, labels,\n",
        "      validation_data=(inputsVal,labels_val),\n",
        "      callbacks=[early_stopping],\n",
        "      batch_size=16,\n",
        "      epochs=epochs)\n",
        "export_dir='./saved_model'\n",
        "tf.saved_model.save(bert_classifier, export_dir=export_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 30522}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "370/500 [=====================>........] - ETA: 3:13 - loss: 0.7158 - accuracy: 0.5338"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7sUoobqjj_0"
      },
      "source": [
        "bert_classifier_model = tf.keras.models.load_model('drive/MyDrive/saved_model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G1IwkRu80YR"
      },
      "source": [
        "result= bert_classifier.predict(inputsTest)\n",
        "result = tf.argmax(result,axis=1).numpy()\n",
        "test_result = []\n",
        "for i in range(len(result)):\n",
        "  id=\"test.en-en.{}\".format(i)\n",
        "  test_result.append({\n",
        "      'id':id,\n",
        "      'tag':'T' if result[i]==1 else 'F'\n",
        "  })\n",
        "json_object = json.dumps(test_result, indent = 4) \n",
        "with open('test.en-en', 'w') as outfile:\n",
        "    outfile.write(json_object) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRrP-kal2f3k"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivkXKLVLqwBA"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i69X6_Spg6J",
        "outputId": "b618633f-0c5d-4a7e-8215-6050462f6bcc"
      },
      "source": [
        "inputs_test=prepareData(test_data)\n",
        "result= bert_classifier.predict(inputs_test)\n",
        "result = tf.argmax(result,axis=1).numpy()\n",
        "# print(test_labels.shape)\n",
        "print(accuracy_score(labels_test,result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0.855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEUian4j23WP"
      },
      "source": [
        "result= bert_classifier(inputsTest,training=False)\n",
        "result = tf.argmax(result,axis=1).numpy()\n",
        "# print(test_labels.shape)\n",
        "print(classification_report(test_labels,result))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}